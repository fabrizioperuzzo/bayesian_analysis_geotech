{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis and modern machine learning algorithms applied for soil and rock probabilistic characterization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern programming languages made easier the use of statistical analysis of large amount of data. \n",
    "The Bayesian analysis is helpful to update knowledge of something.\n",
    "Is particularly indicated when we have to deal with classification. \n",
    "This is why it is helpful when appled to geotechnical material.\n",
    "The instrument we are going to use is pandas packages under python code for what concern the data manipulation.\n",
    "The sklearn package and in particular SNN classification will be used in order to verify the omogeneity of the selected sample with respect to the other material obtained from other sample.\n",
    "Successively we are going to use the package pcm3 for the bayesian analysis\n",
    "An online database by means of Mysql open source will keep storage of the soil material and will be helpful for future analyis.\n",
    "The criteria of storing data will be also the geografical area linked to climate and geological period, local atmosfere (lake, sea, desertic) and load history. This will be helpful to assimilate similar soil.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages to deal with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make extensive use of Pandas packages to deal with data. This packages offer the peculiarity to communicate with a huge amount of data protocols like html, json, xls, db, csv and is constantly update to accomodate recent development like supporting the hdf format that is a development of the csv format but much faster and disk space optimization.\n",
    "The following code show how this package will be use to upload data from the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newenv1\n",
    "#!/usr/bin/env conda run -n pymc3venv python\n",
    "import sys\n",
    "print(sys.executable)    ### Shoul be this C:\\ProgramData\\Anaconda3_1\\envs\\pymc3venv\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --yes --prefix {sys.prefix} m2w64-toolchain\n",
    "# after it add to PATH\n",
    "# ...Anaconda3\\pkgs\\m2w64-gcc-5.3.0-6\\Library\\mingw-w64\\bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota pymc3 requires an old version of numpy to work --> No with 1.16.5 is working\n",
    "# Install a conda package in the current Jupyter kernel\n",
    "# !conda install --yes --prefix {sys.prefix} numpy==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes --prefix {sys.prefix} pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we set pandas options in order to enlarge the row/columns visualization\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 9)\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Scipy helper functions\n",
    "from scipy.stats import percentileofscore\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML Models for comparison\n",
    "from sklearn.linear_model           import LinearRegression\n",
    "from sklearn.linear_model           import ElasticNet\n",
    "from sklearn.ensemble               import RandomForestRegressor\n",
    "from sklearn.ensemble               import ExtraTreesRegressor\n",
    "from sklearn.ensemble               import GradientBoostingRegressor\n",
    "from sklearn.svm                    import SVR\n",
    "\n",
    "from sklearn                        import metrics, svm\n",
    "from sklearn.linear_model           import LogisticRegression\n",
    "from sklearn.tree                   import DecisionTreeClassifier\n",
    "from sklearn.neighbors              import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis  import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes            import GaussianNB\n",
    "from sklearn.svm                    import SVC\n",
    "from sklearn.ensemble               import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training/testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Distributions\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC3 for Bayesian Inference\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('geotech1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheetsls = xls.sheet_names\n",
    "sheetsls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(xls, sheetsls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame by cleaning all missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1.copy()\n",
    "df.columns=df.columns+' '+df.iloc[0,:].astype('str')\n",
    "df.columns = df.columns.str.replace('nan','')\n",
    "df.columns = df.columns.str.replace(' 611','')\n",
    "df.columns = df.columns.str.replace(\"'\",'^')\n",
    "df.columns = df.columns.str.replace(\"%\",'[p]')\n",
    "df.columns = df.columns.str.replace(\"j\",'ϕ')\n",
    "df.drop([0,1], axis='rows', inplace=True)\n",
    "df.dropna(axis='columns', thresh=5, inplace=True)\n",
    "df.dropna(axis='rows', thresh=9, inplace=True)\n",
    "\n",
    "# Select the desired columns\n",
    "\n",
    "df=df[['Zarr0,25 m','σ^v [kPa]', 'gwet ', 'wn ', 'wopt ', 'CaCo3 ',\n",
    "       'Sr [p]', 'LL [p]', 'PI [p]', 'Ic ', 'OCROED [-]', 'OCRTX [-]',\n",
    "       'e0 [-]', 'Cc [-]', 'Ce [-]', '(N1)60 ',\n",
    "       'cTX ', 'ϕTX ', 'c^TX ', 'ϕ^TX ', 'Cu ', 'OCR ',\n",
    "       'cDSS ', 'ϕDSS ', 'CU,VANE ', 'Ep1 Mpa', 'Pl Mpa', 'Em/Pl ', 'WL.1 ', 'PI.1 ',\n",
    "       'Cu,Ucs ']]\n",
    "\n",
    "# correct the columns name\n",
    "\n",
    "df.columns = df.columns.str.replace(\" \",'_')\n",
    "df.columns = df.columns.str.replace('\\[-\\]$','')\n",
    "df.columns = df.columns.str.replace('_$','')\n",
    "\n",
    "# correct the column type and plot statistics\n",
    "\n",
    "df=df.astype('float')\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we notice that:\n",
    "- Water content wn should be expressed in percentage but some data are not (min value 0.27) we will drop values < 1\n",
    "- Sr should be expressed in percentage but is not (maximum value is 1)\n",
    "- OCR and OCRTx are the same column we will replace the column OCR by the mean of the 2\n",
    "\n",
    "The describe function permit to evaluate only data that permits statistics, we will then reduce the dataframe to such columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct wn values \n",
    "df['wn']=df['wn'].apply(lambda x: x if (x>1) else x*100)\n",
    "\n",
    "# joint similar data\n",
    "df['OCR'] = df[[\"OCROED\",'OCRTX']].mean(axis=1)\n",
    "df['LL'] = df[[\"LL_[p]\",'WL.1']].mean(axis=1)\n",
    "df['IP'] = df[[\"PI_[p]\",'PI.1']].mean(axis=1)\n",
    "df=df.drop(columns=[\"PI_[p]\",'PI.1',\"LL_[p]\",'WL.1'])\n",
    "df=df.drop(columns=[\"OCROED\",'OCRTX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df[df.describe().columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rename the columns with suitable names\n",
    "- Drop columns that are not relevant and drop row that have less than 3 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns=['Zarr0,25_m', 'σ^v_[kPa]', 'gwet', 'wn', 'wopt', 'CaCo3', 'Sr', 'Ic', 'e0', 'Cc', 'Ce', '(N1)60', 'c-TX', 'ϕ-Tx', 'c^Tx', 'ϕ^Tx', 'Cu-Tx', 'OCR', 'c-Dss', 'ϕ-Dss', 'CU-Vane', 'Ep1_[Mpa]', 'Pl_[Mpa]', 'Em/Pl', 'Cu-Ucs', 'LL', 'PI']\n",
    "df2.dropna(axis='rows', thresh=3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['count']=df.count(axis=1)\n",
    "df2.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is now ordered. The next step is a population routine that will replace all NaN values by regressed values.\n",
    "Let's order the parameters by count. This will allow us to decide which parameters are going to lead the populate routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd=df2.describe()\n",
    "dfdt = dfd.transpose()\n",
    "dfdt.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe 'dfo' that is ordered according to the most relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfd[['OCR','Cc','Ce','c-TX','ϕ-Tx','c^Tx','ϕ^Tx','Cu-Tx','c-Dss','ϕ-Dss','Cu-Ucs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfd[['OCR','Cc','Ce','c-TX','ϕ-Tx','c^Tx','ϕ^Tx','Cu-Tx','c-Dss','ϕ-Dss','Cu-Ucs']].to_excel(r'data_lab_sec.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfd[['OCR','Cc','Ce','(N1)60','Ep1_[Mpa]','Pl_[Mpa]','Em/Pl','CU-Vane']].to_excel(r'data_lab_third.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.axes_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "corr = dfd.corr()\n",
    "ax.tick_params(labelsize=12)\n",
    "sns.heatmap(corr,\n",
    "           xticklabels=corr.columns.values,\n",
    "           yticklabels=corr.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = df2[dfdt.sort_values('count', ascending=False).index.tolist()].sort_values('count',ascending=False).copy()\n",
    "dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg=dfo.groupby('Zarr0,25_m').mean()\n",
    "print('The shape of the new dataframe is:',dfg.shape)\n",
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_1 = dfg.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the variables are distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling missing data by interpolation\n",
    "\n",
    "One of the possibility to populate missing data is by means of interpolations. SInce we have lots of missing data we are obliged to start by using an unsupervised method of filling. Wi will do this for the following paramters:\n",
    "\n",
    "- Atterberg Limits (LL,IP)\n",
    "- Water content\n",
    "\n",
    "Since interpolation may interfere with regression before doing that we regress the lines where we have most available information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe setting up\n",
    "\n",
    "The final dataframe will be based on:\n",
    "\n",
    "- Insitu stress s'v\n",
    "- Atterberg Limits (LL,IP) \n",
    "- Water content\n",
    "- Corrected Nspt --> (N1)60\n",
    "- Undrained shear strength from Triaxial Test\n",
    "- Undrained shear strength from Vane Test\n",
    "- Limit pressure from pressuremeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we noticed that N160 and Cu-Tx are lognormal distributed we will use the log of Nspt\n",
    "# dfg['(N1)60']=np.log10(dfg['(N1)60'])\n",
    "# dfo['(N1)60']=np.log10(dfo['(N1)60'])\n",
    "# dfg['Cu-Tx']=np.log10(dfg['Cu-Tx'])\n",
    "# dfo['Cu-Tx']=np.log10(dfo['Cu-Tx'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select the relevant columns only\n",
    "\n",
    "dfg=dfg[['σ^v_[kPa]', '(N1)60', 'PI', 'LL', 'wn','CU-Vane','Pl_[Mpa]','Cu-Tx']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values of NSTP by regression procedures\n",
    "\n",
    "We know that the SPT are directly linked with undrained resistance and that this one is also directly linked with Atterberg Limits and water content. We will therefore fill missing (N1)60 values by regression.\n",
    "First we need to check if there are lines that are filled with the wanted parameters:\n",
    "\n",
    "- The regressand : (N1)60\n",
    "- The regressors: 'σ^v_[kPa]', 'PI', 'LL', 'wn'\n",
    "\n",
    "We find that in 36 lines we have full data to perform such regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg['PI'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['LL'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['wn'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['σ^v_[kPa]'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['(N1)60'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['CU-Vane'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['Pl_[Mpa]'].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "dfg['PI'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['LL'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['wn'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['σ^v_[kPa]'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['(N1)60'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['CU-Vane'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "dfg['Pl_[Mpa]'].interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "\n",
    "dfg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we regress SPT with available data we can regress in order to apply the regression on the interpolated data and be able to give completeness to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "plt.plot( 10**dfg['Cu-Tx'],dfg.index, 'b-', label='CU TX')\n",
    "plt.plot( 10**dfo['Cu-Tx'],dfo['Zarr0,25_m'], 'b.', label='CU TX Real',markersize=10)\n",
    "plt.plot( dfg['CU-Vane'],dfg.index, 'r-', label='CU Vane')\n",
    "plt.plot( dfo['CU-Vane'],dfo['Zarr0,25_m'], 'r.', label='CU Vane Real',markersize=10)\n",
    "plt.plot( 10**dfg['(N1)60']*10,dfg.index, 'm-', label='CU SPT Stroud')\n",
    "plt.plot( 10**dfo['(N1)60']*10,dfo['Zarr0,25_m'], 'm.', label='CU SPT Stroud__Real',markersize=10)\n",
    "plt.ylim(60,0); plt.ylabel('depth [m]'); plt.legend(); plt.title('paramters vs depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the database as h5 py format\n",
    "\n",
    "it requires pip intall tables\n",
    "e import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.to_hdf('dfg.h5', key='losses')\n",
    "dfo.to_hdf('dfo.h5', key='losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.read_hdf('dfg.h5', key='losses')\n",
    "dfo = pd.read_hdf('dfo.h5', key='losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running a regression the first thing do do is :\n",
    "\n",
    "- Check the principal rules of validi of LR\n",
    "   - Homoskedaticity\n",
    "   - Errors non correlated with regressor\n",
    "   - ...\n",
    "   - ...\n",
    "- Successively we have to discern the variables that are correlated with the regressand. In fact selecting too many variables may lead to overfitting or may lead to wrong estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.corr()['Cu-Tx'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 24)\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(dfg)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'b')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train the model\n",
    "\n",
    "We will now use the package sklearn that envelop several machine learning algorithms. We will train some of them and we will check wich one is the most performing.\n",
    "\n",
    "First of all we are going to split the data randomly between train and test data, by mean of a method enmbedded in the sklearn package. The parameters are set such that 5% of the data will be kept aside from training and will be used to check the godness of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate a df with simple names otherwise pymc3 formula won't work\n",
    "dfg.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.columns = ['σpv', 'n160', 'pi', 'll', 'wn', 'cuvane', 'pl', 'cutx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.iloc[:,-1:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL WITH 1% OF TRAIN AND REDUCING TO -30m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfg.iloc[:75,:-1], \n",
    "                                                    dfg.iloc[:75,-1:], \n",
    "                                                    test_size = 0.95,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this regression task, we will use two standard metrics:\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\lvert x_{i} - \\bar{x_{i}}\\lvert\n",
    "\\end{equation}$$\n",
    "\n",
    "- Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{i} - \\bar{x_{i}})^2}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mae and rmse\n",
    "def evaluate_predictions(predictions, real):\n",
    "    predictions=np.array(predictions)  # correzione\n",
    "    real=np.array(real)                # correzione\n",
    "    mae = np.mean(abs(predictions - real))\n",
    "    rmse = np.sqrt(np.mean((predictions - real) ** 2))\n",
    "    \n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive baseline is the median\n",
    "median_pred = y_train.median()\n",
    "median_preds = [median_pred for _ in range(len(X_test))]\n",
    "median_preds = pd.DataFrame(median_preds)\n",
    "real = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the naive baseline metrics\n",
    "mb_mae, mb_rmse = evaluate_predictions(median_preds, real)\n",
    "print('Median Baseline  MAE: {:.4f}'.format(mb_mae))\n",
    "print('Median Baseline RMSE: {:.4f}'.format(mb_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate several ml models by training on training set and testing on testing set\n",
    "def evaluate(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Names of models\n",
    "    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n",
    "                      'Random Forest', 'Extra Trees', 'SVM',\n",
    "                       'Gradient Boosted', 'Baseline']\n",
    "    \n",
    "    # Instantiate the models\n",
    "    model1 = LinearRegression()\n",
    "    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "    model3 = RandomForestRegressor(n_estimators=50)\n",
    "    model4 = ExtraTreesRegressor(n_estimators=50)\n",
    "    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n",
    "    model6 = GradientBoostingRegressor(n_estimators=20)\n",
    "    \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n",
    "    \n",
    "    # Train and predict with each model\n",
    "    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        mae = np.mean(abs(predictions - y_test))\n",
    "        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "        \n",
    "        # Insert results into the dataframe\n",
    "        model_name = model_name_list[i]\n",
    "        results.loc[model_name, :] = [mae, rmse]\n",
    "    \n",
    "    # Median Value Baseline Metrics\n",
    "    baseline = np.median(y_train)\n",
    "    baseline_mae = np.mean(abs(baseline - y_test))\n",
    "    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n",
    "    \n",
    "    results.loc['Baseline', :] = [baseline_mae, baseline_rmse]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "# Root mean squared error\n",
    "ax =  plt.subplot(1, 2, 1)\n",
    "results.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax)\n",
    "plt.title('Model Mean Absolute Error'); plt.ylabel('MAE');\n",
    "\n",
    "# Median absolute percentage error\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax)\n",
    "plt.title('Model Root Mean Squared Error'); plt.ylabel('RMSE');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula from Ordinary Least Squares Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(np.array(X_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_ # sono 7 coefficienti attenzione doppio array([[]])\n",
    "X_train.columns.tolist(),y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_[0],X_train.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "ols_formula = 'log10(cutx)=%0.4f+' % lr.intercept_\n",
    "for i, col in enumerate(X_train.columns[:]):\n",
    "    ols_formula += ' %0.4f * %s +' % (lr.coef_[0][i], col)\n",
    "\n",
    "' '.join(ols_formula.split(' ')[:-1]).replace('n160 ','log10(N1,60)') #eliminate last '+' and add Log10 for SPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for Bayesian Linear Regression (follows R formula syntax\n",
    "formula = 'cutx ~ ' + ' + '.join(['%s' % variable for variable in X_train.columns[0:]])\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model in PyMC3 and Sample from Posterior.\n",
    "We now build the model using the formula defined above and a normal distribution for the data likelihood. Then, we let a Markov Chain Monte Carlo algorithm draw samples from the posterior to approximate the posterior for each of the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for the model\n",
    "# pm stands for pymc3 \n",
    "# try st.johnsonsb\n",
    "\n",
    "with pm.Model() as normal_model_fab_student:\n",
    "    \n",
    "    # The prior for the model parameters will be a normal distribution\n",
    "    \n",
    "    family = pm.glm.families.StudentT(df = (int(y_train.shape[0])-1))\n",
    "    \n",
    "    Data=y_train.merge(X_train, left_index=True, right_index=True)\n",
    "    \n",
    "    # Creating the model requires a formula and data (and optionally a family)\n",
    "    pm.GLM.from_formula(formula, data = Data, family = family)\n",
    "    \n",
    "    # Perform Markov Chain Monte Carlo sampling\n",
    "    normal_trace = pm.sample(draws=2000, chains = 2, tune = 500) #draws=2000, chains = 2, tune = 500\n",
    "    \n",
    "\n",
    "####  BELOW IS THE CODE FOR GAUSSIAN NORMAL INFERENCE\n",
    "\n",
    "# with pm.Model() as normal_model:\n",
    "    \n",
    "#     # The prior for the model parameters will be a normal distribution\n",
    "#     family = pm.glm.families.Normal()\n",
    "    \n",
    "#     Data=y_train.merge(X_train, left_index=True, right_index=True)\n",
    "    \n",
    "#     # Creating the model requires a formula and data (and optionally a family)\n",
    "#     pm.GLM.from_formula(formula, data = Data, family = family)\n",
    "    \n",
    "#     # Perform Markov Chain Monte Carlo sampling\n",
    "#     normal_trace = pm.sample(draws=2000, chains = 2, tune = 500) #draws=2000, chains = 2, tune = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the trace with a vertical line at the mean of the trace\n",
    "def plot_trace(trace):\n",
    "    # Traceplot with vertical lines at the mean value\n",
    "    ax = pm.traceplot(trace, figsize=(14, len(trace.varnames)*1.8),\n",
    "                      lines={k: v['mean'] for k, v in pm.summary(trace).iterrows()})\n",
    "    \n",
    "    matplotlib.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Labels with the median value\n",
    "    for i, mn in enumerate(pm.summary(trace)['mean']):\n",
    "        ax[i, 0].annotate('{:0.4f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,\n",
    "                          xytext = (-18, 18), textcoords = 'offset points', rotation = 90,\n",
    "                          va = 'bottom', fontsize = 'large', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace(normal_trace);\n",
    "# or the embedded function is : pm.traceplot(normal_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arviz as az\n",
    "# se non funziona il plot sopra\n",
    "# az.plot_trace(normal_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(normal_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(normal_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'Log10(Cu-Tx) = '\n",
    "for variable in normal_trace.varnames:\n",
    "    model_formula += ' %0.4f * %s +' % (np.mean(normal_trace[variable]), variable)\n",
    "\n",
    "' '.join(model_formula.split(' ')[:-1]).replace('n160 ','log10(N1,60)') #eliminate last '+' and add Log10 for SPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We compare mae and rmse with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pm.summary(normal_trace).iloc[:-1,0]\n",
    "\n",
    "clmn_coeff = coeffs.index.tolist()\n",
    "\n",
    "X_test_ = X_test.copy()\n",
    "\n",
    "X_test_[clmn_coeff[0]]=1\n",
    "X_test_ = X_test_[clmn_coeff].copy()  # ordino il dataframe con lo stesso ordine del trace\n",
    "\n",
    "resultsbayes = pd.DataFrame(index = X_test.index, columns = ['estimate'])\n",
    "\n",
    "for row in X_test_.iterrows():\n",
    "        resultsbayes.loc[row[0], 'estimate'] = np.dot(np.array(coeffs), np.array(row[1]))\n",
    "        \n",
    "resultsbayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(resultsbayes.estimate)\n",
    "\n",
    "# Metrics\n",
    "mae = np.mean(abs(predictions - np.array(y_test)))\n",
    "rmse = np.sqrt(np.mean((predictions - np.array(y_test)) ** 2))\n",
    "\n",
    "# Insert results into the dataframe\n",
    "model_name = 'Bayesian lr'\n",
    "results.loc[model_name, :] = [mae, rmse]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "# Root mean squared error\n",
    "ax =  plt.subplot(1, 2, 1)\n",
    "results.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax)\n",
    "plt.title('Model Mean Absolute Error'); plt.ylabel('MAE');\n",
    "\n",
    "# Median absolute percentage error\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax)\n",
    "plt.title('Model Root Mean Squared Error'); plt.ylabel('RMSE');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new prediction from the test set and compare to actual value\n",
    "def test_model(trace, test_observation):\n",
    "    \n",
    "    # Print out the test observation data\n",
    "    print('Test Observation:')\n",
    "    print(test_observation)\n",
    "    \n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "\n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    \n",
    "    # Standard deviation of the likelihood\n",
    "    sd_value = var_weights['sd'].mean()\n",
    "\n",
    "    # Actual Value\n",
    "    actual = test_observation['cutx']\n",
    "    \n",
    "    # Add in intercept term\n",
    "    test_observation['Intercept'] = 1\n",
    "    test_observation = test_observation.drop('cutx').copy()\n",
    "    \n",
    "    # Align weights and test observation\n",
    "    var_weights = var_weights[test_observation.index]\n",
    "\n",
    "    # Means for all the weights\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "\n",
    "    # Location of mean for observation\n",
    "    mean_loc = np.dot(var_means, test_observation)\n",
    "    \n",
    "    # Estimates of grade\n",
    "    estimates = np.random.normal(loc = mean_loc, scale = sd_value,\n",
    "                                 size = 1000)\n",
    "\n",
    "    # Plot all the estimates\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n",
    "                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n",
    "                kde_kws = {'linewidth' : 4},\n",
    "                label = 'Estimated Dist.')\n",
    "    # Plot the actual grade\n",
    "    plt.vlines(x = actual, ymin = 0, ymax = 5, \n",
    "               linestyles = '--', colors = 'red',\n",
    "               label = 'True cutx',\n",
    "              linewidth = 2.5)\n",
    "    \n",
    "    # Plot the mean estimate\n",
    "    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n",
    "               linestyles = '-', colors = 'orange',\n",
    "               label = 'Mean Estimate',\n",
    "              linewidth = 2.5)\n",
    "    \n",
    "    plt.legend(loc = 1)\n",
    "    plt.title('Density Plot for Test Observation');\n",
    "    plt.xlabel('Grade'); plt.ylabel('Density');\n",
    "    \n",
    "    # Prediction information\n",
    "    print('True cutx = %d' % actual)\n",
    "    print('Average Estimate = %0.4f' % mean_loc)\n",
    "    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n",
    "                                       np.percentile(estimates, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test=y_test.merge(X_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(normal_trace, Data_test.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Variable Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function pm.plot_posterior_predictive_glm to see the effect in changing one variable. This takes a range of values to use for the variable, a linear model, and a number of samples. The function evaluates the linear model across the range of values for the number of samples. Each time, it draws a different set of parameters from the trace. This gives us an indication of the effect of a single variable and also the uncertainty in the model estimates. To see the effect of a single variable, we hold the others constant at their median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examines the effect of changing a single variable\n",
    "# Takes in the name of the variable, the trace, and the data\n",
    "\n",
    "def model_effect(query_var, trace, X):\n",
    "    \n",
    "    # Variables that do not change\n",
    "    steady_vars = list(X.columns)\n",
    "    steady_vars.remove(query_var)\n",
    "    \n",
    "    # Linear Model that estimates a grade based on the value of the query variable \n",
    "    # and one sample from the trace\n",
    "    def lm(value, sample):\n",
    "        \n",
    "        # Prediction is the estimate given a value of the query variable\n",
    "        prediction = sample['Intercept'] + sample[query_var] * value\n",
    "        \n",
    "        # Each non-query variable is assumed to be at the median value\n",
    "        for var in steady_vars:\n",
    "            \n",
    "            # Multiply the weight by the median value of the variable\n",
    "            prediction += sample[var] * X[var].median()\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Find the minimum and maximum values for the range of the query var\n",
    "    var_min = X[query_var].min()\n",
    "    var_max = X[query_var].max()\n",
    "    \n",
    "    # Plot the estimated grade versus the range of query variable\n",
    "    pm.plot_posterior_predictive_glm(trace, eval=np.linspace(var_min, var_max, 100), \n",
    "                                     lm=lm, samples=100, color='blue', \n",
    "                                     alpha = 0.4, lw = 2)\n",
    "    \n",
    "    # Plot formatting\n",
    "    plt.xlabel('%s' % query_var, size = 16)\n",
    "    plt.ylabel('cutx', size = 16)\n",
    "    plt.title(\"Posterior of cutx vs %s\" % query_var, size = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('ll', normal_trace, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('σpv', normal_trace, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('pi', normal_trace, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some variation between the t-distribution estimates and those from the normal distribution for the data likelihood.Choosing appropriate priors is one of the hardest aspect of Bayesian Modeling, but we can get around that by having more data. As the amount of data the model learns from increases, the prior has less of an effect because each time the posterior is updated based on the new data. Essentially machine learning models perform inference with no priors, basing the final model entirely on the data. In the case of limited samples, Bayesian Inference can be a better method for building models because it provides a reasonable estimate in situations with few data points (as long as the prior is reasonable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we looked at using Bayesian Linear Regression to predict student performance based on six factors. Rather than specify probabilities for the Bayesian network which is basically impossible for continuous variables, we framed the problem as a machine learning task. In addition to the standard machine learning models that learn from observations, we also used Bayesian Linear Regression to create a model mapping the features (student characteristics) to the targets (final grade). The advantages of Bayesian Linear Regression are that if we use sensible priors, we can still get a decent estimate with few samples, and the final weights are not a single number, but a distribution componsed of every sample drawn during the sampling run. We can then make predictions using all the sampled weights to form a distribution of expected values rather than a single answer. \n",
    "\n",
    "The Bayesian  Linear Regression did not perform as well as the other methods in terms of the two metrics we choose. This might not be the ideal case for a Bayesian inference approach but we saw that Bayesian Linear Regression produced intuitive estimates for the model weights and gave predictions for new students that align with our expectations for the factors influencing student performance. To summarize, although Bayesian Linear Regression did not outperform the standard machine learning methods, it gave us a chance to learn another tool for use in evaluating and making sense of data. It's always a positive to have more skills that you can deploy as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Redo the job by plotting multy graph without log(spt) and log(cutx) in order to present the exploratory data analysis speaking about that graph\n",
    "- Redo the analysis missing data by relating all the variables to s'v or 'z'\n",
    "- Redo the analysis missing data by doing only the interpolate function\n",
    "- Use the best from the above\n",
    "- This is to solve the problem of inverse correlated cu-vane with cu-tx and inverse with pl that is strange\n",
    "- Redo the analysis with bayesian t-student distribution\n",
    "- Show that the Bayesian anlysis is the best we can use we we do not dispose of a lot of data\n",
    "- Take a large amount of data and do the linear regression that will show a very low error estimate\n",
    "- Take a small part of the X-test and use it to perform1:\n",
    "    - Bayesian analysis\n",
    "    - Linear Regression\n",
    "- Show that Bayesian is better than linear regression when we do not dispose of a lot of data\n",
    "    - Redo the analysis by changing the splittin part \n",
    "    \n",
    "- Redo the Pymc3 analysis with student\n",
    "- Search a better distribution for cutx with automatic distribution fitter \n",
    "- Redo the Pymc3 analysis with the best fit distribution\n",
    "- Try not to fill missing data but using a limited number of row but with full data each\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
